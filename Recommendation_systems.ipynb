{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics modified by popularity: It's hard to say whether we should place a movie rated 9 by 100000 users below another moview rated 9.5 by 1000 users. A weighted rating could be applied:\n",
    "$$\\text{WR} = (\\frac{v}{v+m} \\times R) + (\\frac{m}{v+m} \\times C)ï¼Œ$$\n",
    "where $v$ is the number of votes for the specific movie, $m$ is the minimum number of votes required for movies to be in the rank list, $R$ is the mean rating of the specific movie and $C$ is the mean rating of all the movies in the dataset. Therefore, for a specific moviev, less votes means that the rating will be closer to the average rating. Meanwhile, more votes means that the rating will approach a value that is reflective of the specific\n",
    "movie's quality and popularity with the general populace.\n",
    "\n",
    "$m$ is to be selected subjectively (e.g. 80th percentile, must have collected more votes than at least\n",
    "$80\\%$ of the movies present in the dataset), higher the value of m, the higher the emphasis on the popularity of a movie. \n",
    "\n",
    "Considering to impose other restrictions such as duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge-based recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the user for the features of movies he/she is looking for (e.g. genres, range of duration, release date......). Filter the dataset with the information collected. Recalculate $m$ and $C$. Finally, recommend moviews to the user that have a high weighted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based recommenders: ask users for a few favorite movies and recommend results that are similar to those movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two vertorization method:\n",
    "1. CountVectorizer\n",
    "\n",
    "Count the total number of unique words in all documents we are intereted in with the extremely common words such as \n",
    "a, the, is, had, my... being ignored. Represent each docunment as a numerical vector with each element being the  number of times each recorded unique words occurs. (Very similar to creating dummy variables)\n",
    "2. TF-IDFVectorizer\n",
    "\n",
    "For every word $i$ in document $j$, applies the following equation to get the weight of word $i$ in documnet $j$:\n",
    "$$w_{i, j} = tf_{i, j} \\times \\text{log}(\\frac{N}{df_i}),$$\n",
    "where $tf_{i, j}$ is the total number of occurences of word $i$ in document $j$, $df_i$ is the number of documents that contain the word $i$. $N$ is the total number of documents.\n",
    "\n",
    "So the weight of a word in a document is greater if it occurs more frequently in that document and is present in fewer other documents.\n",
    "\n",
    "\n",
    "Similarity measure\n",
    "1. The cosine similarity score (particularly useful when applied in conjunction with TF-IDFVectorizer)\n",
    "\n",
    "Given two documents $x$ and $y$, $\\text{cos}(x, y) = \\frac{x^Ty}{||x||_2 ||y||_2}$ measures the similarity between them. The closer the cosine score to $\\pm 1$, the more similar the documents are to each other.\n",
    "\n",
    "\n",
    "One commonly used submodel: Metadata-based recommender\n",
    "\n",
    "Employ several features together such as genre, director, three major stars, keywords, etc. Since there are several features to work with, we need to create a \"soup\" that contains all the features (most of them are strings or lists of strings). In that way, we can feed the soup into selected Vectorizer and calculate similarity scores. \n",
    "\n",
    "In this model, it's better to use CountVectorizer. Because TF-IDFVectorizer would give actors and directors who have acted and directed in a relatively larger number of movies less weight and this is not desirable. The similarity contributed by the actors' and directors' name is important in this case.\n",
    "\n",
    "\n",
    "Improvements? See reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative-filtering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering demands data on user behavior. We might want to download the movieLens data https://www.kaggle.com/prajitdatta/movielens-100k-dataset, which contains the demographic information of 1000 users. We could gather the user ids, movie ids, demographic information per user, rating per movie and user into one dataframe.\n",
    "\n",
    "\n",
    "1. User-based collaborative filtering: find users similar to a particular user and then recommend products that those users have liked to the first user.\n",
    "\n",
    "First represent the ratings by a matrix where each row represents a user and each column represents a movie. Then the following models are worthwhile to try:\n",
    "\n",
    "(a). Baseline model: outputs the mean rating for the movie by all the users who have rated it. If the rating for some movies are available only in the test dataset, then just default to $3.0$.\n",
    "\n",
    "(b). Weighted mean: Give more weights to those users whose ratings are similar to the user we are predicting based on the following formula:\n",
    "$$r_{u,m} = \\frac{\\sum_{u' \\neq u} \\text{sim}(u, u') r_{u', m}}{\\sum_{u' \\neq u}  \\text{sim}(u, u')},$$\n",
    "where $r_{u,m}$ represents the rating given by user u to movie m and $\\text{sim}(\\cdot)$ is the similarity measure.\n",
    "\n",
    "The above two models did not include demographic information: How about we include some demographic features when computing the similarity measure and then proceed to use the same formula?\n",
    "\n",
    "2. Item-based collaborative\n",
    "\n",
    "Compute the pairwise similarity of every item in the inventory (e.g. movie) instead of every user. Just like transpose the rating matrix. \n",
    "\n",
    "3. Model-based approaches (machine learning)\n",
    "Clustering, PCA, XGboost, neural networks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
